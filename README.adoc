= Code Readability Classifier

This Python program utilizes machine learning to predict the readability of source code snippets.
It is designed to work with Python 3.11.5 and uses Poetry for package management.

The most recent implementation of the model is made with https://keras.io/[keras] (see `keas` folder).
Previously, we tried to achieve the same with https://pytorch.org/[pytorch] (see `toch` folder), but we did not achieve the same classification accuracy for unknown reasons.

The model is based on the https://github.com/swy0601/Readability-Features/tree/master[implementation] of the following paper:

----
@article{mi2022towards,
  title={Towards using visual, semantic and structural features to improve code readability classification},
  author={Mi, Qing and Hao, Yiqun and Ou, Liwei and Ma, Wei},
  journal={Journal of Systems and Software},
  volume={193},
  pages={111454},
  year={2022},
  publisher={Elsevier}
}
----

== Table of Contents

* <<Installation>>
* <<Usage>>
** <<Predict>>
** <<Train>>
* <<Dataset>>
* <<Podman>>
* <<Model_Overview>>

[[Installation]]
== Installation

To set up the project and its dependencies, follow these steps:

Clone this repository to your local machine:

[source,bash]
----
git clone https://github.com/LuKrO2011/readability-classifier
cd readability-classifier
----

Install Poetry if you haven't already:

[source,bash]
----
pip install poetry
----

Create a virtual environment and install the project's dependencies using Poetry:

[source,bash]
----
poetry install
----

Activate the virtual environment:

[source,bash]
----
poetry shell
----

Install tensorflow manually:

----
pip install tensorflow
----

For Developers: Activate the pre-commit hooks:

----
pre-commit install
----

Now you're ready to use the source code readability prediction tool.

[[Usage]]
== Usage

[[Predict]]
=== Predict

To predict the readability of a source code snippet, use the following command:

[source,bash]
----
python src/readability_classifier/main.py PREDICT --model MODEL --input INPUT [--token-length TOKEN_LENGTH]
----

* `--model` or `-m`: Path to the pre-trained machine learning model.
* `--input` or `-i`: Path to the source code snippet you want to evaluate.
* `--token-length` or `-l` (optional): The token length of the snippet (cutting/padding applied).

Example:

[source,bash]
----
python src/readability_classifier/main.py PREDICT --model tests/res/models/towards.h5 --input <snippet_path>
----

[[Train]]
=== Train

To train a new machine learning model for source code readability prediction, use the following command:

[source,bash]
----
python src/readability_classifier/main.py TRAIN --input INPUT [--save SAVE] [--intermediate INTERMEDIATE] [--evaluate] [--token-length TOKEN_LENGTH] [--batch-size BATCH_SIZE] [--epochs EPOCHS] [--learning-rate LEARNING_RATE]
----

* `--input` or `-i`: Path to the dataset containing a folder 'Snippets' and 'scores.csv'.
* `--save` or `-s` (optional): Path to the folder where the trained model should be stored.
If not specified, the model is not stored.
* `--intermediate` (optional): Path to the folder where intermediate results should be stored.
If not specified, the intermediate results are not stored.
* `--evaluate` (optional): Whether to evaluate the model after training.
* `--token-length` or `-l` (optional): The token length of the snippets (cutting/padding applied).
* `--batch-size` or `-b` (optional): The batch size for training.
* `--epochs` or `-e` (optional): The number of epochs for training.
* `--learning-rate` or `-r` (optional): The learning rate for training.

Example:

[source,bash]
----
python src/readability_classifier/main.py TRAIN --input tests/res/raw_datasets/bw --save <output-path>
----

[[Dataset]]
== Dataset

The datasets used for training and evaluation are from the following sources:

* BW: Raymond PL Buse and Westley R Weimer.
‘Learning a metric for code readability’
* Dorn: Jonathan Dorn.
‘A general software readability model’.
* Scalabrio: Simone Scalabrino et al.
‘Improving code readability models with textual features’.

You can find the three datasets *merged* into one on https://huggingface.co/datasets/se2p/code-readability-merged[Huggingface].

* Krodinger: Lukas Krodinger ‘Advancing Code Readability: Mined & Modified Code for Dataset Generation‘.

You can also find this *mined-and-modified* dataset on https://huggingface.co/datasets/se2p/code-readability-merged[Huggingface].
The code for the  https://github.com/LuKrO2011/readability-decreasing-heuristics[dataset generation] of the mined-and-modified dataset is also available on GitHub.

[[Podman]]
== Podman

To build the podman container, run the following command:

[source,bash]
----
podman build -t readability-classifier .
----

- t : name of the container
- . : path to the Dockerfile

To run the podman container, run the following command:

[source,bash]
----
podman run -it --rm -v $(pwd):/app readability-classifier
----

- it : interactive mode
- rm : remove container after exit
- v $(pwd):/app : mount current directory to /app in container
- readability-classifier : name of the container

[[Model_Overview]]
== Model Overview

|===
|Layer (type) |Output Shape              |Param # |Connected to

|struc_input (InputLayer) |[(None, 50, 305)]         |0       |[]
|struc_reshape (Reshape) |(None, 50, 305, 1)         |0       |['struc_input[0][0]']
|vis_input (InputLayer) |[(None, 128, 128, 3)]      |0       |[]
|struc_conv1 (Conv2D) |(None, 48, 303, 32)         |320     |['struc_reshape[0][0]']
|vis_conv1 (Conv2D) |(None, 128, 128, 32)        |896     |['vis_input[0][0]']
|struc_pool1 (MaxPooling2D) |(None, 24, 151, 32)    |0       |['struc_conv1[0][0]']
|seman_input_token (InputLayer) |[(None, 100)]    |0       |[]
|seman_input_segment (InputLayer) |[(None, 100)] |0       |[]
|vis_pool1 (MaxPooling2D) |(None, 64, 64, 32)        |0       |['vis_conv1[0][0]']
|struc_conv2 (Conv2D) |(None, 22, 149, 32)          |9248    |['struc_pool1[0][0]']
|seman_bert (BertEmbedding) |(None, 100, 768)       |2342553 |['seman_input_token[0][0]', 'seman_input_segment[0][0]']
|vis_conv2 (Conv2D) |(None, 64, 64, 32)           |9248    |['vis_pool1[0][0]']
|struc_pool2 (MaxPooling2D) |(None, 11, 74, 32)      |0       |['struc_conv2[0][0]']
|seman_conv1 (Conv1D) |(None, 96, 32)               |122912 |['seman_bert[0][0]']
|vis_pool2 (MaxPooling2D) |(None, 32, 32, 32)        |0       |['vis_conv2[0][0]']
|struc_conv3 (Conv2D) |(None, 9, 72, 64)            |18496  |['struc_pool2[0][0]']
|seman_pool1 (MaxPooling1D) |(None, 32, 32)          |0       |['seman_conv1[0][0]']
|vis_conv3 (Conv2D) |(None, 32, 32, 64)           |18496  |['vis_pool2[0][0]']
|struc_pool3 (MaxPooling2D) |(None, 3, 24, 64)        |0       |['struc_conv3[0][0]']
|seman_conv2 (Conv1D) |(None, 28, 32)               |5152   |['seman_pool1[0][0]']
|vis_pool3 (MaxPooling2D) |(None, 16, 16, 64)        |0       |['vis_conv3[0][0]']
|struc_flatten (Flatten) |(None, 4608)               |0       |['struc_pool3[0][0]']
|seman_gru (Bidirectional) |(None, 64)               |16640  |['seman_conv2[0][0]']
|vis_flatten (Flatten) |(None, 16384)                |0       |['vis_pool3[0][0]']
|concatenate (Concatenate) |(None, 21056)             |0       |['struc_flatten[0][0]', 'seman_gru[0][0]', 'vis_flatten[0][0]']
|class_dense1 (Dense) |(None, 64)                   |1347648 |['concatenate[0][0]']
|class_dropout (Dropout) |(None, 64)                 |0       |['class_dense1[0][0]']
|class_dense2 (Dense) |(None, 16)                   |1040   |['class_dropout[0][0]']
|class_dense3 (Dense) |(None, 1)                    |17     |['class_dense2[0][0]']

|===

Total params: 24975649 (95.27 MB)
